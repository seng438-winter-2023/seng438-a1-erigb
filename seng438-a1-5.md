>**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 5  |
|----------------------|
| Ethan Rigby          |
| Labib Afsar Ahmed    |
| Mohammed Alshoura    |
| Danielle Jourdain    |

**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

Before this lab, no one in the group was familiar with the process for manual functional testing. Through previous coding experience both in classes and outside we had become familiar with the process of exploratory testing. Although we did not know it had a formal name and process, we had tested systems we had made previously using this method. We also had not created defect reports before, although we had informally documented issues we had found in our programs. Much like exploratory testing, we were informally familiar with regression testing. When we had found bugs in our previous projects, after fixing them, we would test the same functionality again to see if the issue had been resolved.

# High-level description of the exploratory testing plan

Firsly, the program should be run as intended to familiarize ourselves with the functionality. This will firstly allow us to understand how the program should work. It will also allow us to confirm that the program works as inteded when using valid inputs. We will test logging in with a valid card and PIN. Then we will test depositing and withdrawing valid amounts of money from the accounts. We will also test if a transaction can be cancelled by the user using the cancel button. Lastly, we will attempt to perform balance inquiry transactions on valid user accounts. After becoming familiar with the intended path of the program, we will begin testing the system with invalid inputs.

For the first invalid input, we will try to log in with an invalid PIN and a valid card, followed by an invalid card number. Afterwards, we will log in with one of the valid accounts and attempt to withdraw more money than available from one of the accounts. We will also attempt to withdraw more money than the ATM has even though the user has enough funds available. The last test we will perform will be entering an invalid PIN 3 times to lock a card and ensure the user is unable to take the card back.

Through our exploratory testing, we hope to test most functions a little bit to see if the system can perform the overall funcitonality it describes as well as determine the use cases to test during the manual functional testing phase.

# Comparison of exploratory and manual functional testing

Both exploratory and manual testing revealed various bugs about the program. During exploratory testing, hunting for bugs felt disorganized and chaotic. Even though we had come up with a plan for our testing, hunting for bugs felt like there was very little order to our work. When we moved onto manual functional testing, the process felt much more organized. During the manual testing, we found many more issues with the system. Since we were tesing every functionality of the system, we were able to find bugs in any spot they were hiding. Overall, exploratory testing is good for getting familiar with the system and how it functions. In contrast, manual functional testing is better for actually finding issues with the program in a more logical way.

# Notes and discussion of the peer reviews of defect reports

Using peer coding, we were able to effectively examine the program and find many bugs. We were able to focus our strengths as team members. One team member was able to focus on checking the system thoroughly while the other was focused on documenting the complete issue and how to recreate it. One function that helped us with peer testing was the reporter and assignee roles in Jira. This helped each peer review group to keep track of the errors that we found while communicating them with each other.

After we had done this peer coding, we moved onto the defect reports. Each student tracked the bugs they found and added the report using Jira. Once we had all completed our testing, we all reviewed all bug reports that had been made. We ensured they all had the same level of detail about the issue. This ensured we were consistent about how to recreate the issues and therefore made it easy to understand. We also made sure all functions being tested were clearly explained. We also ensured that both the expected and actual output of every bug was clearly stated. By using peer review we were able to ensure that our reports were consistent and matched all of our ideas.

# How the pair testing was managed and team work/effort was divided

During pair testing, we broke into two teams of two. First each team took some time to become familiar with the system. Afterwards, one team decided to test valid inputs while the other team tested invalid inputs. During pair testing we split the work in half further, so each person tested and documented at one point.

Afterwards, when we moved onto the manual scripted testing, each team member took 10 of the use cases from Appendix C and tested them. Once we had completed that and had documented all of the bugs, we first ensured all of the documentation was consistent. Finally, we moved on to regression testing, where each member tested the same 10 use cases in version 1.1.

# Difficulties encountered, challenges overcome, and lessons learned

Overall, the testing was successful. However, the exploratory testing felt disorganized. This showed us that although exploratory testing is good for understanding the system, it is not great for acutally finding bugs. During manual scripted testing, we found many more bugs.

Ensuring we tested every corner of the program was a large challenge. It was difficult to ensure that we found every bug in the system. Using a more structured approach to testing programs is a much more efficient way to find bugs in applications. It allows for groups to test programs much easier while ensuring the whole program gets thouroughly tested.

# Comments/feedback on the lab and lab document itself

Overall the lab was a great introduction to the testing process. Everyone on the team got a chance to look for bugs as well as complete defect reports. Everyone was involved in every step of the process which made the lab very hands on. However, the instructions were unclear and some of the expectations were also unclear. More clear lab documents about what to do and what needs to be handed in would be appreciated in the future. It would be ideal to have a sample lab report and defect report given to us so that we know what is expected of us and would take out a lot of uncertainty.
